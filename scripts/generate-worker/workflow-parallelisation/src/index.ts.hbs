import { Hono } from "hono";
import { cors } from "hono/cors";
import type { Env } from "./types/env.ts";
import type { Variables } from "./types/hono.ts";
import { authApiKey } from "../../../libs/middleware/src/auth-api-key";
import { createWorkersAI } from "workers-ai-provider";
import { generateObject } from "ai";
import z from "zod";

const app = new Hono<{ Bindings: Env; Variables: Variables }>();
app.use(cors());
app.use("*", authApiKey);
// Schema for each small LLM output.
// Each model returns a string field "angleOutput" representing its perspective.
const angleSchema = z.object({
	angleOutput: z.string(),
});

// Schema for the aggregator's final output.
// The aggregator returns a comprehensive result as a single string.
const finalOutputSchema = z.object({
	finalResult: z.string(),
});

app.post("/", async (c) => {
	// Extract the initial prompt from the request body.
	const { prompt } = (await c.req.json()) as { prompt: string };

	const workersai = createWorkersAI({ binding: c.env.AI });
	const bigModel = workersai("@cf/meta/llama-3.3-70b-instruct-fp8-fast");
	const smallModel = workersai("@cf/meta/llama-3.1-8b-instruct-fp8");

	// --- Step 1: Execute Parallel Calls (Voting) ---
	// Prepare three distinct prompts to get diverse perspectives on the same input.
	const anglePrompts = [
		`Angle 1: Analyse the following prompt from a technical perspective and provide a detailed response:\n\n${prompt}`,
		`Angle 2: Analyse the following prompt from a creative perspective and provide a detailed response:\n\n${prompt}`,
		`Angle 3: Analyse the following prompt from a user-centric perspective and provide a detailed response:\n\n${prompt}`,
	];

	// Execute the three small LLM calls concurrently using Promise.all.
	const smallLLMCalls = anglePrompts.map((anglePrompt) =>
		generateObject({
			model: smallModel,
			schema: angleSchema,
			prompt: anglePrompt,
		}),
	);
	const angleResults = await Promise.all(smallLLMCalls);

	// Extract the outputs from each call.
	const angleOutputs = angleResults.map((result) => result.object.angleOutput);

	// --- Step 2: Aggregation via a Larger LLM ---
	// Build a prompt for the aggregator that incorporates all three responses.
	const aggregatorPrompt = `The following responses provide diverse perspectives on a given prompt:\n\n
		${angleOutputs
			.map((output, index) => `Response ${index + 1}: ${output}`)
			.join("\n\n")}
		\n\nBased on these responses, please synthesise a comprehensive final result.
		Return your answer as a JSON object in the format { "finalResult": "Your comprehensive result here." }`;

	// Invoke the aggregator LLM.
	const { object: aggregatorResult } = await generateObject({
		model: bigModel,
		schema: finalOutputSchema,
		prompt: aggregatorPrompt,
	});

	// Return the aggregated result along with the individual perspectives.
	return c.json({
		angleOutputs,
		aggregatorResult,
	});
});

export default {
	fetch: app.fetch,
} satisfies ExportedHandler<Env>;
